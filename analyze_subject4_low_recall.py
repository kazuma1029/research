# analyze_subject4_low_recall.py
# è¢«é¨“è€…4ã®ã¿å†ç¾ç‡ãŒä½ã„ç†ç”±ã‚’åˆ†æ
# -*- coding: utf-8 -*-

import os
import json
import pandas as pd
import numpy as np
from pathlib import Path

def load_pr_metrics(subject, n=300):
    """æŒ‡å®šè¢«é¨“è€…ãƒ»Nã® PR ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    pr_path = f"./å®Ÿé¨“çµæœ/è¢«é¨“è€…{subject}GPT/ææ¡ˆæ‰‹æ³•1/{n}/sub{subject}_run1_mode2_ranktrain_N{n}_pr_table.xlsx"
    
    try:
        df = pd.read_excel(pr_path)
        return df
    except:
        return None

def load_predictions(subject, n=300):
    """äºˆæ¸¬çµæœã‚’èª­ã¿è¾¼ã¿"""
    pred_path = f"./å®Ÿé¨“çµæœ/è¢«é¨“è€…{subject}GPT/ææ¡ˆæ‰‹æ³•1/{n}/sub{subject}_run1_mode2_ranktrain_N{n}_predictions.xlsx"
    
    try:
        df = pd.read_excel(pred_path)
        return df
    except:
        return None

def load_rubric(subject, n=300):
    """ãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯ã‚’èª­ã¿è¾¼ã¿"""
    rubric_path = f"./å®Ÿé¨“çµæœ/è¢«é¨“è€…{subject}GPT/ææ¡ˆæ‰‹æ³•1/{n}/sub{subject}_run1_mode2_ranktrain_N{n}_rubric.txt"
    
    try:
        with open(rubric_path, 'r', encoding='utf-8') as f:
            return f.read()
    except:
        return None

def load_learning_data(subject):
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    liked_path = f"./ãƒ©ãƒ³ã‚­ãƒ³ã‚°/{subject}_liked_reviews.txt"
    disliked_path = f"./ãƒ©ãƒ³ã‚­ãƒ³ã‚°/{subject}_disliked_reviews.txt"
    
    liked = []
    disliked = []
    
    try:
        with open(liked_path, 'r', encoding='utf-8') as f:
            liked = [ln.strip() for ln in f if ln.strip()][1:]
    except:
        pass
    
    try:
        with open(disliked_path, 'r', encoding='utf-8') as f:
            disliked = [ln.strip() for ln in f if ln.strip()][1:]
    except:
        pass
    
    return liked, disliked

def analyze_recall_comparison():
    """å…¨è¢«é¨“è€…ã®å†ç¾ç‡ã‚’æ¯”è¼ƒ"""
    
    print("=" * 100)
    print("åˆ†æ: è¢«é¨“è€…åˆ¥ã®å†ç¾ç‡æ¯”è¼ƒï¼ˆN=300, ææ¡ˆæ‰‹æ³•1ï¼‰")
    print("=" * 100)
    
    print("\nã€1ã€‘å†ç¾ç‡ï¼ˆRecallï¼‰ã®æ¯”è¼ƒ\n")
    print(f"{'è¢«é¨“è€…':<10} {'Precision':<15} {'Recall':<15} {'F1':<15} {'PRä¹–é›¢':<12}")
    print("-" * 80)
    
    results = {}
    max_f1_thresholds = {}  # å„è¢«é¨“è€…ã®æœ€å¤§F1æ™‚ã®é–¾å€¤
    
    for subject in [1, 2, 3, 4, 5]:
        df_pr = load_pr_metrics(subject, 300)
        
        if df_pr is None:
            print(f"{subject:<10} {'ï¼ˆèª­ã¿è¾¼ã¿å¤±æ•—ï¼‰':<15}")
            continue
        
        # æœ€å¤§F1ã®è¡Œã‚’å–å¾—
        max_f1_idx = df_pr['f1'].idxmax()
        max_f1_row = df_pr.loc[max_f1_idx]
        
        p = float(max_f1_row['precision'])
        r = float(max_f1_row['recall'])
        f1 = float(max_f1_row['f1'])
        thr = float(max_f1_row['threshold'])
        gap = abs(p - r)
        
        print(f"{subject:<10} {p:<15.4f} {r:<15.4f} {f1:<15.4f} {gap:<12.4f}")
        
        results[subject] = {
            'p': p,
            'r': r,
            'f1': f1,
            'threshold': thr,
            'gap': gap,
            'pr_table': df_pr
        }
        max_f1_thresholds[subject] = thr
    
    # è¢«é¨“è€…4ã®ç‰¹ç•°æ€§ã‚’å¼·èª¿
    print("\n")
    if 4 in results:
        recall_4 = results[4]['r']
        other_recalls = [results[s]['r'] for s in [1, 2, 3, 5] if s in results]
        avg_recall_others = np.mean(other_recalls)
        
        print(f">>> è¢«é¨“è€…4ã®å†ç¾ç‡ãŒä½ã„:")
        print(f"    è¢«é¨“è€…4: {recall_4:.4f}")
        print(f"    ä»–è¢«é¨“è€…å¹³å‡: {avg_recall_others:.4f}")
        print(f"    ä½ä¸‹é‡: {avg_recall_others - recall_4:.4f} ({100*(avg_recall_others - recall_4)/avg_recall_others:.1f}%)")
    
    return results, max_f1_thresholds

def analyze_score_distribution(results):
    """ã‚¹ã‚³ã‚¢åˆ†å¸ƒã‚’åˆ†æ"""
    
    print("\n\nã€2ã€‘ã‚¹ã‚³ã‚¢åˆ†å¸ƒã®æ¯”è¼ƒï¼ˆæ­£ä¾‹ã¨è² ä¾‹ã®åˆ†é›¢åº¦ï¼‰\n")
    print(f"{'è¢«é¨“è€…':<10} {'æ­£ä¾‹ã‚¹ã‚³ã‚¢å¹³å‡':<15} {'è² ä¾‹ã‚¹ã‚³ã‚¢å¹³å‡':<15} {'åˆ†é›¢åº¦':<12} {'ã‚¹ã‚³ã‚¢SD':<12}")
    print("-" * 80)
    
    score_stats = {}
    
    for subject in [1, 2, 3, 4, 5]:
        df_pred = load_predictions(subject, 300)
        
        if df_pred is None:
            print(f"{subject:<10} {'ï¼ˆèª­ã¿è¾¼ã¿å¤±æ•—ï¼‰':<15}")
            continue
        
        y_true = df_pred['y_true'].values
        scores = df_pred['score'].values
        
        pos_scores = scores[y_true == 1]
        neg_scores = scores[y_true == 0]
        
        pos_mean = np.mean(pos_scores) if len(pos_scores) > 0 else np.nan
        neg_mean = np.mean(neg_scores) if len(neg_scores) > 0 else np.nan
        
        # ã‚¯ãƒ©ã‚¹é–“åˆ†é›¢åº¦ï¼ˆä¿¡å·å¯¾ãƒã‚¤ã‚ºæ¯”ï¼‰
        separation = abs(pos_mean - neg_mean) / (np.std(pos_scores) + np.std(neg_scores) + 1e-8)
        overall_std = np.std(scores)
        
        print(f"{subject:<10} {pos_mean:<15.4f} {neg_mean:<15.4f} {separation:<12.4f} {overall_std:<12.4f}")
        
        score_stats[subject] = {
            'pos_mean': pos_mean,
            'neg_mean': neg_mean,
            'separation': separation,
            'std': overall_std,
            'pos_scores': pos_scores,
            'neg_scores': neg_scores,
            'y_true': y_true,
            'scores': scores
        }
    
    # è¢«é¨“è€…4ã®ç‰¹ç•°æ€§
    print("\n")
    if 4 in score_stats:
        sep_4 = score_stats[4]['separation']
        other_seps = [score_stats[s]['separation'] for s in [1, 2, 3, 5] if s in score_stats]
        avg_sep_others = np.mean(other_seps)
        
        print(f">>> è¢«é¨“è€…4ã®ã‚¯ãƒ©ã‚¹åˆ†é›¢åº¦ãŒä½ã„:")
        print(f"    è¢«é¨“è€…4: {sep_4:.4f}")
        print(f"    ä»–è¢«é¨“è€…å¹³å‡: {avg_sep_others:.4f}")
        print(f"    ä½ä¸‹ç‡: {100*(1 - sep_4/avg_sep_others):.1f}%")
    
    return score_stats

def analyze_rubric_quality(results):
    """ãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯å“è³ªã‚’åˆ†æ"""
    
    print("\n\nã€3ã€‘ãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯ç‰¹å¾´ã®æ¯”è¼ƒ\n")
    print(f"{'è¢«é¨“è€…':<10} {'ç·ç‰¹å¾´æ•°':<15} {'Aç¾¤ç‰¹å¾´æ•°':<15} {'Bç¾¤ç‰¹å¾´æ•°':<15}")
    print("-" * 60)
    
    rubric_stats = {}
    
    for subject in [1, 2, 3, 4, 5]:
        rubric_text = load_rubric(subject, 300)
        
        if rubric_text is None:
            print(f"{subject:<10} {'ï¼ˆèª­ã¿è¾¼ã¿å¤±æ•—ï¼‰':<15}")
            continue
        
        try:
            rubric_json = json.loads(rubric_text)
            
            # ç‰¹å¾´æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            total_features = 0
            group_a_features = 0
            group_b_features = 0
            
            for key, val in rubric_json.items():
                if isinstance(val, list):
                    count = len(val)
                    total_features += count
                    if 'ç¾¤A' in key or 'Aç¾¤' in key:
                        group_a_features = count
                    elif 'ç¾¤B' in key or 'Bç¾¤' in key:
                        group_b_features = count
            
            print(f"{subject:<10} {total_features:<15} {group_a_features:<15} {group_b_features:<15}")
            
            rubric_stats[subject] = {
                'total': total_features,
                'group_a': group_a_features,
                'group_b': group_b_features,
                'rubric': rubric_json
            }
        except json.JSONDecodeError:
            print(f"{subject:<10} {'ï¼ˆJSONè§£æå¤±æ•—ï¼‰':<15}")
            rubric_stats[subject] = {'error': True}
    
    # è¢«é¨“è€…4ã®ç‰¹ç•°æ€§
    print("\n")
    if 4 in rubric_stats and 'error' not in rubric_stats[4]:
        total_4 = rubric_stats[4]['total']
        other_totals = [rubric_stats[s]['total'] for s in [1, 2, 3, 5] if s in rubric_stats and 'error' not in rubric_stats[s]]
        avg_total_others = np.mean(other_totals)
        
        print(f">>> è¢«é¨“è€…4ã®ãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯ç‰¹å¾´æ•°:")
        print(f"    è¢«é¨“è€…4: {total_4} å€‹")
        print(f"    ä»–è¢«é¨“è€…å¹³å‡: {avg_total_others:.1f} å€‹")
        print(f"    å·®åˆ†: {total_4 - avg_total_others:+.1f} å€‹")
    
    return rubric_stats

def analyze_learning_data_characteristics():
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã‚’åˆ†æ"""
    
    print("\n\nã€4ã€‘å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰ã®ç‰¹æ€§\n")
    print(f"{'è¢«é¨“è€…':<10} {'å¥½ããªæ˜ ç”»æ•°':<15} {'å«Œã„ãªæ˜ ç”»æ•°':<15} {'ãƒãƒ©ãƒ³ã‚¹':<15} {'ãƒ¬ãƒ“ãƒ¥ãƒ¼é•·å¹³å‡':<15}")
    print("-" * 80)
    
    learning_stats = {}
    
    for subject in [1, 2, 3, 4, 5]:
        liked, disliked = load_learning_data(subject)
        
        n_liked = len(liked)
        n_disliked = len(disliked)
        balance = min(n_liked, n_disliked) / max(n_liked, n_disliked) if max(n_liked, n_disliked) > 0 else 0
        
        # ãƒ¬ãƒ“ãƒ¥ãƒ¼é•·ã®å¹³å‡
        all_reviews = liked + disliked
        avg_len = np.mean([len(r.split()) for r in all_reviews]) if all_reviews else 0
        
        print(f"{subject:<10} {n_liked:<15} {n_disliked:<15} {balance:<15.3f} {avg_len:<15.1f}")
        
        learning_stats[subject] = {
            'n_liked': n_liked,
            'n_disliked': n_disliked,
            'balance': balance,
            'avg_len': avg_len,
            'liked': liked,
            'disliked': disliked
        }
    
    # è¢«é¨“è€…4ã®ç‰¹ç•°æ€§
    print("\n")
    if 4 in learning_stats:
        stat_4 = learning_stats[4]
        other_balances = [learning_stats[s]['balance'] for s in [1, 2, 3, 5] if s in learning_stats]
        avg_balance_others = np.mean(other_balances)
        
        print(f">>> è¢«é¨“è€…4ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§:")
        print(f"    ã‚¯ãƒ©ã‚¹ãƒãƒ©ãƒ³ã‚¹: {stat_4['balance']:.3f}")
        print(f"    ä»–è¢«é¨“è€…å¹³å‡: {avg_balance_others:.3f}")
        
        if stat_4['balance'] < avg_balance_others:
            print(f"    â†’ ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ãŒå¤§ãã„ï¼ˆå½±éŸ¿ã‚ã‚Šï¼‰")
        
        other_avg_len = np.mean([learning_stats[s]['avg_len'] for s in [1, 2, 3, 5] if s in learning_stats])
        print(f"\n    ãƒ¬ãƒ“ãƒ¥ãƒ¼é•·ï¼ˆå˜èªæ•°ï¼‰: {stat_4['avg_len']:.1f}")
        print(f"    ä»–è¢«é¨“è€…å¹³å‡: {other_avg_len:.1f}")
        
        if stat_4['avg_len'] < other_avg_len:
            print(f"    â†’ ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒçŸ­ã„å¯èƒ½æ€§ï¼ˆãƒã‚¤ã‚ºãŒç›¸å¯¾çš„ã«å¤§ãã„ï¼‰")
    
    return learning_stats

def analyze_why_subject4_low_recall(score_stats, rubric_stats, learning_stats):
    """è¢«é¨“è€…4ã®å†ç¾ç‡ãŒä½ã„ç†ç”±ã‚’è©³ç´°åˆ†æ"""
    
    print("\n\nã€5ã€‘è¢«é¨“è€…4ã®å†ç¾ç‡ãŒä½ã„ç†ç”±ï¼ˆè©³ç´°è€ƒå¯Ÿï¼‰\n")
    
    if 4 not in score_stats:
        print("è¢«é¨“è€…4ã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        return
    
    print("è¦³å¯Ÿã•ã‚ŒãŸè¢«é¨“è€…4ã®ç‰¹å¾´:")
    print("-" * 80)
    print()
    
    # 1. ã‚¹ã‚³ã‚¢åˆ†å¸ƒã®å•é¡Œ
    if score_stats[4]['separation'] < np.mean([score_stats[s]['separation'] for s in [1,2,3,5] if s in score_stats]):
        print("1. âŒ ã‚¯ãƒ©ã‚¹åˆ†é›¢åº¦ãŒä½ã„")
        print("   åŸå› :")
        print("   - LLMãŒæŠ½å‡ºã—ãŸãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯ãŒã€è¢«é¨“è€…4ã®ãƒ‡ãƒ¼ã‚¿ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ãªã„")
        print("   - æ­£ä¾‹ã¨è² ä¾‹ã®ã‚¹ã‚³ã‚¢ãŒéåº¦ã«é‡ãªã£ã¦ã„ã‚‹")
        print("   - çµæœï¼šã©ã®é–¾å€¤ã‚’é¸æŠã—ã¦ã‚‚ã€å†ç¾ç‡ã¨ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒæ‚ªã„")
        print()
    
    # 2. ãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯å“è³ªã®å•é¡Œ
    if 4 in rubric_stats and 'error' not in rubric_stats[4]:
        total_4 = rubric_stats[4]['total']
        other_totals = [rubric_stats[s]['total'] for s in [1, 2, 3, 5] if s in rubric_stats and 'error' not in rubric_stats[s]]
        
        if total_4 < np.mean(other_totals):
            print("2. âŒ ãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯ç‰¹å¾´æ•°ãŒå°‘ãªã„")
            print("   åŸå› :")
            print("   - è¢«é¨“è€…4ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸå—œå¥½ç‰¹å¾´ãŒé™å®šçš„")
            print("   - ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã«ä½¿ç”¨ã§ãã‚‹æƒ…å ±ãŒä¸è¶³")
            print("   - çµæœï¼šã‚¹ã‚³ã‚¢ã®ç²¾åº¦ãŒä½ä¸‹ã—ã€åˆ†é¡ãŒã§ããªããªã‚‹")
            print()
    
    # 3. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§
    if 4 in learning_stats:
        stat_4 = learning_stats[4]
        
        if stat_4['balance'] < 0.8:
            print("3. âŒ ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡")
            print("   åŸå› :")
            print(f"   - å¥½ããªæ˜ ç”»ã¨å«Œã„ãªæ˜ ç”»ã®æ•°ãŒå¤§ããç•°ãªã‚‹ï¼ˆ{stat_4['n_liked']} vs {stat_4['n_disliked']}ï¼‰")
            print("   - LLMãŒãƒã‚¸ãƒ§ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã«åã‚Šã‚„ã™ããªã‚‹")
            print("   - çµæœï¼šãƒã‚¤ãƒãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ï¼ˆæ­£ä¾‹ï¼‰ã®å†ç¾ç‡ãŒç‰¹ã«ä½ä¸‹")
            print()
        
        if stat_4['avg_len'] < 50:  # å˜èªæ•°ã®ä¸€ä¾‹
            print("4. âŒ ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒçŸ­ã„")
            print("   åŸå› :")
            print(f"   - å¹³å‡ãƒ¬ãƒ“ãƒ¥ãƒ¼é•·ãŒçŸ­ã„ï¼ˆ{stat_4['avg_len']:.1f}å˜èªï¼‰")
            print("   - çŸ­ã„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§ã¯å—œå¥½ã®è©³ç´°ãŒè¡¨ç¾ã•ã‚Œãªã„")
            print("   - LLMã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ååˆ†ãªå—œå¥½æƒ…å ±ãŒå¾—ã‚‰ã‚Œãªã„")
            print("   - çµæœï¼šãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯æŠ½å‡ºã®ç²¾åº¦ãŒä½ä¸‹")
            print()
    
    print("5. ğŸ” BERT ã¨ã®æ¯”è¼ƒã§åˆ†ã‹ã‚‹ã“ã¨ã¯ï¼Ÿ")
    print("   - BERT: ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã§ã®å¾®ç´°ãªç‰¹å¾´ã‚’è‡ªå‹•å­¦ç¿’")
    print("           ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã‚„ãƒ‡ãƒ¼ã‚¿å“è³ªã®å½±éŸ¿ã‚’å—ã‘ã«ãã„")
    print("   ")
    print("   - ChatGPTï¼ˆLLMï¼‰: ãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯æŠ½å‡ºã«ä¾å­˜")
    print("           å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®è³ªãƒ»é‡ãŒç›´æ¥çš„ã«ç²¾åº¦ã«å½±éŸ¿")
    print("           è¢«é¨“è€…4ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ®Šæ€§ãŒé¡•è‘—ã«å‡ºã‚„ã™ã„")
    print()

def generate_recommendations():
    """æ”¹å–„ææ¡ˆ"""
    
    print("\n\nã€6ã€‘æ”¹å–„ææ¡ˆ\n")
    
    print("è¢«é¨“è€…4ã®å†ç¾ç‡ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã®æ–½ç­–:")
    print("-" * 80)
    print()
    
    print("1. ãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯æŠ½å‡ºã®æ”¹å–„")
    print("   - Few-shot ä¾‹ã‚’è¢«é¨“è€…4ç”¨ã«æœ€é©åŒ–")
    print("   - ã‚ˆã‚Šè©³ç´°ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å—œå¥½ã‚’å¼•ãå‡ºã™")
    print("   - LLMã® temperature ã‚’èª¿æ•´ï¼ˆæ¢ç´¢æ€§ã¨å®‰å®šæ€§ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰")
    print()
    
    print("2. ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ”¹å–„")
    print("   - è¢«é¨“è€…4ã®è©•ä¾¡è»¸ã«åˆã‚ã›ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹æˆ")
    print("   - é–¾å€¤ã‚’æœ€å¤§F1ã§ã¯ãªãã€å†ç¾ç‡é‡è¦–ã§æ±ºå®šã™ã‚‹")
    print()
    
    print("3. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿é‡ã®èª¿æ•´")
    print("   - N=300 ãŒå¸¸ã«æœ€é©ã¨ã¯é™ã‚‰ãªã„")
    print("   - è¢«é¨“è€…4ã«ã¤ã„ã¦ã¯ N=200 ã‚„ N=400 ã‚’è©¦ã™")
    print("   - ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’å„ªå…ˆï¼ˆé‡ã‚ˆã‚Šè³ªï¼‰")
    print()
    
    print("4. ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã¸ã®å¯¾ç­–")
    print("   - class_weight='balanced' ã‚’ä½¿ç”¨ï¼ˆæ—¢ã«å®Ÿè£…æ¸ˆã¿ï¼‰")
    print("   - ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° or ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°")
    print("   - ã‚«ã‚¹ã‚¿ãƒ æå¤±é–¢æ•°ï¼ˆå†ç¾ç‡é‡è¦–ï¼‰")
    print()
    
    print("5. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ã®æ¤œè¨")
    print("   - BERT ãƒ™ãƒ¼ã‚¹ã®åˆ†é¡ã¨ã®èåˆ")
    print("   - æœ€çµ‚çš„ãªåˆ¤æ–­ã‚’è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æŠ•ç¥¨ã§æ±ºå®š")
    print("   - ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®ä½ã„åˆ¤æ–­ã¯äººé–“ãƒ¬ãƒ“ãƒ¥ãƒ¼")
    print()

def main():
    print("\n")
    
    # åˆ†æå®Ÿè¡Œ
    results, max_f1_thresholds = analyze_recall_comparison()
    score_stats = analyze_score_distribution(results)
    rubric_stats = analyze_rubric_quality(results)
    learning_stats = analyze_learning_data_characteristics()
    
    # è©³ç´°è€ƒå¯Ÿ
    analyze_why_subject4_low_recall(score_stats, rubric_stats, learning_stats)
    
    # æ”¹å–„ææ¡ˆ
    generate_recommendations()
    
    print("\n" + "=" * 100)
    print("ã¾ã¨ã‚")
    print("=" * 100)
    print()
    print("è¢«é¨“è€…4ã®ã¿å†ç¾ç‡ãŒä½ã„ç†ç”±:")
    print()
    print("â‘  ã‚¯ãƒ©ã‚¹åˆ†é›¢åº¦ã®ä½ä¸‹")
    print("   â†’ ãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯å“è³ªã¾ãŸã¯ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ€é©åŒ–ä¸è¶³")
    print()
    print("â‘¡ ãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯ç‰¹å¾´æ•°ã®ä¸è¶³")
    print("   â†’ LLM ãŒè¢«é¨“è€…4ã®å—œå¥½ã‚’ååˆ†ã«æŠ½å‡ºã§ãã¦ã„ãªã„")
    print()
    print("â‘¢ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ï¼ˆä¸å‡è¡¡ãƒ»çŸ­ã•ãªã©ï¼‰")
    print("   â†’ LLM ãƒ™ãƒ¼ã‚¹æ‰‹æ³•ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®è³ªã«æ•æ„Ÿ")
    print()
    print("â‘£ BERT ã¨ã®æ€§èƒ½å·®")
    print("   â†’ BERT ã¯å¾®ç´°ãªç‰¹å¾´ã‚’è‡ªå‹•å­¦ç¿’ã™ã‚‹ãŸã‚ã€å …ç‰¢æ€§ãŒé«˜ã„")
    print("   â†’ LLM ã¯æ˜ç¤ºçš„ãªæŒ‡ç¤ºã«ä¾å­˜ã™ã‚‹ãŸã‚ã€å€‹åˆ¥æœ€é©åŒ–ãŒå¿…è¦")
    print()
    print("â‡’ æ¨å¥¨: è¢«é¨“è€…4å‘ã‘ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å€‹åˆ¥èª¿æ•´ã™ã‚‹")
    print("\n" + "=" * 100)

if __name__ == "__main__":
    main()
